{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9663ad9-217c-46d3-befc-8ccabeba890b",
   "metadata": {},
   "source": [
    "# Document parsing and chuncking technique\n",
    "\n",
    "\n",
    "One of the most effective strategies to improve performance of your language model applications is to split your large data into smaller pieces. This is call splitting or chunking In the world of multi-modal, splitting also applies to images.\n",
    "\n",
    "**Evaluations**\n",
    "\n",
    "It's important to test the chunking strategies in retrieval evals. It doesn't matter how you chunk if the performance of your application isn't great.\n",
    "\n",
    "Eval Frameworks:\n",
    "\n",
    "* [LangChain Evals](https://python.langchain.com/docs/guides/evaluation/)\n",
    "* [Llama Index Evals](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html)\n",
    "* [RAGAS Evals](https://github.com/explodinggradients/ragas)\n",
    "\n",
    "\n",
    "## 1: Character Splitting <a id=\"CharacterSplitting\"></a>\n",
    "Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character sized chunks regardless of their content or form.\n",
    "\n",
    "This method isn't recommended for any applications - but it's a great starting point for us to understand the basics.\n",
    "\n",
    "* **Pros:** Easy & Simple\n",
    "* **Cons:** Very rigid and doesn't take into account the structure of your text\n",
    "\n",
    "Concepts to know:\n",
    "* **Chunk Size** - The number of characters you would like in your chunks. 50, 100, 100,000, etc.\n",
    "* **Chunk Overlap** - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks.\n",
    "\n",
    "First let's get some sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c96299fc-30f5-4edf-ac23-23a29f9c7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1cf67e-98d7-48bd-9867-f72be72e3f4a",
   "metadata": {},
   "source": [
    "Then let's split this text manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f11fb88f-17ed-44c2-b4de-a8a527fe63c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the text I would like to ch',\n",
       " 'unk up. It is the example text for ',\n",
       " 'this exercise']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list that will hold your chunks\n",
    "chunks = []\n",
    "\n",
    "chunk_size = 35 # Characters\n",
    "\n",
    "# Run through the a range with the length of your text and iterate every chunk_size you want\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140085b7-c6af-4003-923c-73feb1825965",
   "metadata": {},
   "source": [
    "When working with text in the language model world, we don't deal with raw strings. It is more common to work with documents. Documents are objects that hold the text you're concerned with, but also additional metadata which makes filtering and manipulation easier later.\n",
    "\n",
    "We could convert our list of strings into documents, but I'd rather start from scratch and create the docs.\n",
    "\n",
    "Let's load up LangChains `CharacterSplitter` to do this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d85945f0-4a09-4bd9-bdb6-bafe03089053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f3c0a-09c9-45a9-8f47-d28baf22b201",
   "metadata": {},
   "source": [
    "Then let's load up this text splitter. I need to specify `chunk overlap` and `separator` or else we'll get funk results. We'll get into those next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3dcbeb8d-c5a0-4047-8250-967313c20935",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='', strip_whitespace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae25bbe-d7d1-44da-820b-3cd34a1cfc67",
   "metadata": {},
   "source": [
    "Then we can actually split our text via `create_documents`. Note: `create_documents` expects a list of texts, so if you just have a string (like we do) you'll need to wrap it in `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "afe4945b-ce08-49aa-a5dc-65a0e59922f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is the text I would like to ch'),\n",
       " Document(page_content='unk up. It is the example text for '),\n",
       " Document(page_content='this exercise')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331025f4-6ef4-4459-bcb6-df7824e78ce4",
   "metadata": {},
   "source": [
    "Notice how this time we have the same chunks, but they are in documents. These will play nicely with the rest of the LangChain world. Also notice how the trailing whitespace on the end of the 2nd chunk is missing. This is because LangChain removes it, see [this line](https://github.com/langchain-ai/langchain/blob/f36ef0739dbb548cabdb4453e6819fc3d826414f/libs/langchain/langchain/text_splitter.py#L167) for where they do it. You can avoid this with `strip_whitespace=False`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0f193-4098-4fb1-a42f-7f96cd182188",
   "metadata": {},
   "source": [
    "**Chunk Overlap & Separators**\n",
    "\n",
    "**Chunk overlap** will blend together our chunks so that the tail of Chunk #1 will be the same thing and the head of Chunk #2 and so on and so forth.\n",
    "\n",
    "This time I'll load up my overlap with a value of 4, this means 4 characters of overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc66f496-7b0d-4b2a-a43d-e8f06d58c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=4, separator='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fd5d7e36-b592-430e-9069-cc025c78d7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is the text I would like to ch'),\n",
       " Document(page_content='o chunk up. It is the example text'),\n",
       " Document(page_content='ext for this exercise')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4aaa8-b90b-499e-b2d5-bc623b5bb751",
   "metadata": {},
   "source": [
    "Notice how we have the same chunks, but now there is overlap between 1 & 2 and 2 & 3. The 'o ch' on the tail of Chunk #1 matches the 'o ch' of the head of Chunk #2.\n",
    "\n",
    "**Separators** are character(s) sequences you would like to split on. Say you wanted to chunk your data at `ch`, you can specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "814ce9aa-17c3-4205-b433-2eae612c2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='ch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bb759b1f-dab0-4f5e-a0c0-220374313da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is the text I would like to'),\n",
       " Document(page_content='unk up. It is the example text for this exercise')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7ff4a-b6f0-4924-9a9c-2791dddc5b37",
   "metadata": {},
   "source": [
    "#### Llama Index\n",
    "\n",
    "[Llama Index](https://www.llamaindex.ai/) is a great choice for flexibility in the chunking and indexing process. They provide node relationships out of the box which can aid in retrieval later.\n",
    "\n",
    "Let's take a look at their sentence splitter. It is similar to the character splitter, but using its default settings, it'll split on sentences instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "624832d6-f3fd-45c4-b52e-bbc86e5e3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e77494-a253-4414-b32d-2246fcb396ef",
   "metadata": {},
   "source": [
    "Load up your splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ef9e6627-81a5-463d-9180-5ff2ff1d40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SentenceSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23214df1-064d-47ad-9058-f9a5f113b6cf",
   "metadata": {},
   "source": [
    "Load up your document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7cafba29-b973-4b37-806f-6f4893eae02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/mit.txt\"]\n",
    ").load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ceae08-1f5e-46bc-9d98-e95182fe8c3c",
   "metadata": {},
   "source": [
    "Create your nodes. Nodes are similar to documents but with more relationship data added to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "52f9a2e2-3d56-4727-afce-c81c13089324",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec353aac-ea8f-421a-9895-0af8afdc08e0",
   "metadata": {},
   "source": [
    "Then let's take a look at one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "898a421f-54a0-45a4-8acb-c7087b6a883f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='e2da252d-b9de-4f35-8331-1820e05a95df', embedding=None, metadata={'file_path': 'data/mit.txt', 'file_name': 'mit.txt', 'file_type': 'text/plain', 'file_size': 36045, 'creation_date': '2024-04-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='05f680af-b91f-4048-8563-109eb15daa4c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'data/mit.txt', 'file_name': 'mit.txt', 'file_type': 'text/plain', 'file_size': 36045, 'creation_date': '2024-04-21', 'last_modified_date': '2024-04-13'}, hash='9276085c940fea4ded70180ba5921621f434437b7a70d64e4b989d6c2a01f1ab'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='72699c88-30c9-490f-be5f-a72ba404ab89', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c2b8f9ce71859fbb29975890bcf2586ab730687cf0ab966b9be05f9c00c59737')}, text=\"Want to start a startup?  Get funded by\\nY Combinator.\\n\\n\\n\\n\\nOctober 2006(This essay is derived from a talk at MIT.)\\nTill recently graduating seniors had two choices: get a job or go\\nto grad school.  I think there will increasingly be a third option:\\nto start your own startup.  But how common will that be?I'm sure the default will always be to get a job, but starting a\\nstartup could well become as popular as grad school.  In the late\\n90s my professor friends used to complain that they couldn't get\\ngrad students, because all the undergrads were going to work for\\nstartups.  I wouldn't be surprised if that situation returns, but\\nwith one difference: this time they'll be starting their own\\ninstead of going to work for other people's.The most ambitious students will at this point be asking: Why wait\\ntill you graduate?\", start_char_idx=2, end_char_idx=823, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539aa1a2-67b8-4585-b0d3-306703ea856b",
   "metadata": {},
   "source": [
    "As you can see there is a lot more relationship data held within Llama Index's nodes. We'll talk about those later, I don't want to get ahead of ourselves\n",
    "\n",
    "Basic Character splitting is likely only useful for a few applications, maybe yours!\n",
    "\n",
    "## 2: Recursive Character Text Splitting\n",
    "<a id=\"RecursiveCharacterSplitting\"></a>\n",
    "Let's jump a level of complexity.\n",
    "\n",
    "The problem with Level #1 is that we don't take into account the structure of our document at all. We simply split by a fix number of characters.\n",
    "\n",
    "The Recursive Character Text Splitter helps with this. With it, we'll specify a series of separatators which will be used to split our docs.\n",
    "\n",
    "You can see the default separators for LangChain [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L842). Let's take a look at them one by one.\n",
    "\n",
    "* \"\\n\\n\" - Double new line, or most commonly paragraph breaks\n",
    "* \"\\n\" - New lines\n",
    "* \" \" - Spaces\n",
    "* \"\" - Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "49f42bea-3d06-404d-9f8c-f15f7ff7591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f79f4-769b-474b-8d7d-19cb48407cd6",
   "metadata": {},
   "source": [
    "Then let's load up a larger piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0772695d-0c5e-4e19-bb69-14e9bd7a15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb158c-6bbe-4f49-95df-a8b43965a566",
   "metadata": {},
   "source": [
    "Now let's make our text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "03ec54c4-bda6-4254-97dd-983775b1d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 65, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "887c7676-1e67-4084-94d3-59689eb399c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"One of the most important things I didn't understand about the\"),\n",
       " Document(page_content='world when I was a child is the degree to which the returns for'),\n",
       " Document(page_content='performance are superlinear.'),\n",
       " Document(page_content='Teachers and coaches implicitly told us the returns were linear.'),\n",
       " Document(page_content='\"You get out,\" I heard a thousand times, \"what you put in.\" They'),\n",
       " Document(page_content='meant well, but this is rarely true. If your product is only'),\n",
       " Document(page_content=\"half as good as your competitor's, you don't get half as many\"),\n",
       " Document(page_content='customers. You get no customers, and you go out of business.'),\n",
       " Document(page_content=\"It's obviously true that the returns for performance are\"),\n",
       " Document(page_content='superlinear in business. Some think this is a flaw of'),\n",
       " Document(page_content='capitalism, and that if we changed the rules it would stop being'),\n",
       " Document(page_content='true. But superlinear returns for performance are a feature of'),\n",
       " Document(page_content=\"the world, not an artifact of rules we've invented. We see the\"),\n",
       " Document(page_content='same pattern in fame, power, military victories, knowledge, and'),\n",
       " Document(page_content='even benefit to humanity. In all of these, the rich get richer.'),\n",
       " Document(page_content='[1]')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa00043-1655-4113-bb28-f3a998d5713a",
   "metadata": {},
   "source": [
    "Notice how now there are more chunks that end with a period \".\". This is because those likely are the end of a paragraph and the splitter first looks for double new lines (paragraph break).\n",
    "\n",
    "Once paragraphs are split, then it looks at the chunk size, if a chunk is too big, then it'll split by the next separator. If the chunk is still too big, then it'll move onto the next one and so forth.\n",
    "\n",
    "For text of this size, let's split on something bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6da8734e-47da-4a08-8459-9bf8bfed7fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\"),\n",
       " Document(page_content='Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor\\'s, you don\\'t get half as many customers. You get no customers, and you go out of business.'),\n",
       " Document(page_content=\"It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\")]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 450, chunk_overlap=0)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e99768f-8732-44e4-b8d8-cc5ac1fe4661",
   "metadata": {},
   "source": [
    "For this text, 450 splits the paragraphs perfectly. You can even switch the chunk size to 469 and get the same splits. This is because this splitter builds in a bit of cushion and wiggle room to allow your chunks to 'snap' to the nearest separator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f32a73-0c8a-498c-a3a1-3e7dba4658c9",
   "metadata": {},
   "source": [
    "## 3: Document Specific Splitting <a id=\"DocumentSpecific\"></a>\n",
    "\n",
    "Stepping up our levels ladder, let's start to handle document types other than normal prose in a .txt. What if you have pictures? or a PDF? or code snippets?\n",
    "\n",
    "Our first two levels wouldn't work great for this so we'll need to find a different tactic.\n",
    "\n",
    "This level is all about making your chunking strategy fit your different data formats. Let's run through a bunch of examples of this in action\n",
    "\n",
    "The Markdown, Python, and JS splitters will basically be similar to Recursive Character, but with different separators.\n",
    "\n",
    "See all of LangChains document splitters [here](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter) and Llama Index ([HTML](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#htmlnodeparser), [JSON](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#jsonnodeparser), [Markdown](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#markdownnodeparser))\n",
    "\n",
    "### Markdown\n",
    "\n",
    "You can see the separators [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1175).\n",
    "\n",
    "Separators:\n",
    "* `\\n#{1,6}` - Split by new lines followed by a header (H1 through H6)\n",
    "* ```` ```\\n ```` - Code blocks\n",
    "* `\\n\\\\*\\\\*\\\\*+\\n` - Horizontal Lines\n",
    "* `\\n---+\\n` - Horizontal Lines\n",
    "* `\\n___+\\n` - Horizontal Lines\n",
    "* `\\n\\n` Double new lines\n",
    "* `\\n` - New line\n",
    "* `\" \"` - Spaces\n",
    "* `\"\"` - Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "298fe868-0872-4fa9-9146-fa33e9dd5706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e1d390ed-d046-44f9-a492-9760141f7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1ba14168-451b-4e9c-b1d0-d1eac6996ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# Fun in California\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "15dcf8de-551a-4477-8e68-57c4c50ddbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Fun in California\\n\\n## Driving'),\n",
       " Document(page_content='Try driving on the 1 down to San Diego'),\n",
       " Document(page_content='### Food'),\n",
       " Document(page_content=\"Make sure to eat a burrito while you're\"),\n",
       " Document(page_content='there'),\n",
       " Document(page_content='## Hiking\\n\\nGo to Yosemite')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.create_documents([markdown_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "66edcde5-1e96-4b61-8636-8129d31d7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2afa8f93-6b07-484f-86ff-9836f5a5fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_text = \"\"\"\n",
    "class Person:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "for i in range(10):\n",
    "    print (i)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6e8fcc85-714d-4b5c-a5ce-a3f30cfb447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a7b6dd89-6bb9-496a-a85d-3f1871ff9cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='class Person:\\n  def __init__(self, name, age):\\n    self.name = name\\n    self.age = age'),\n",
       " Document(page_content='p1 = Person(\"John\", 36)\\n\\nfor i in range(10):\\n    print (i)')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_splitter.create_documents([python_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab2014-e705-4ade-87ea-d967a9c01593",
   "metadata": {},
   "source": [
    "### PDFs w/ tables\n",
    "\n",
    "Ok now things will get a bit spicier.\n",
    "\n",
    "PDFs are an extremely common data type for language model work. Often they'll contain tables that contain information.\n",
    "\n",
    "This could be financial data, studies, academic papers, etc.\n",
    "\n",
    "Trying to split tables by a character based separator isn't reliable. We need to try out a different method. For a deep dive on this \n",
    "\n",
    "A very convenient way to do this is with [Unstructured](https://unstructured.io/), a library dedicated to making your data LLM ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ffd078d0-5651-4ab0-b299-b2ed5a4f7cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import elements_to_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174a72b-0d43-4440-9ef8-f5f3aed0c651",
   "metadata": {},
   "source": [
    "Let's load up our PDF and then parition it. This is a PDF from a [Salesforce earning report](https://investor.salesforce.com/financials/default.aspx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2ae9042d-be6f-411a-8835-bda30ffa0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/SalesforceFinancial.pdf\"\n",
    "\n",
    "# Extracts the elements from the PDF\n",
    "elements = partition_pdf(\n",
    "    filename=filename,\n",
    "\n",
    "    # Unstructured Helpers\n",
    "    strategy=\"hi_res\", \n",
    "    infer_table_structure=True, \n",
    "    model_name=\"yolox\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ad413-80f9-43da-8fb5-3a32373c3686",
   "metadata": {},
   "source": [
    "Let's look at our elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0f5ac388-5b4e-4dcd-bf74-84220c8cdff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.NarrativeText at 0x77303477e9e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x77300c4a6ad0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x77300c4a42b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x773028cabac0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x773028cab7c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x773028cabb80>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x773028cabeb0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x773028caa950>,\n",
       " <unstructured.documents.elements.Title at 0x77300c400fd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x77303477f4f0>,\n",
       " <unstructured.documents.elements.Table at 0x77300c3d0370>,\n",
       " <unstructured.documents.elements.Text at 0x77303477fca0>,\n",
       " <unstructured.documents.elements.Text at 0x77303477fc10>,\n",
       " <unstructured.documents.elements.Title at 0x77300c4a4910>,\n",
       " <unstructured.documents.elements.Title at 0x77300c4a76a0>,\n",
       " <unstructured.documents.elements.Text at 0x77300c4a6e30>]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a081974-002c-4060-9d0c-2d4a5f270044",
   "metadata": {},
   "source": [
    "These are just unstructured objects, we could look at them all but I want to look at the table it parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9c5460db-689d-4e7a-a5bc-a10477c4a61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table><thead><th></th><th>Q2 FY23 Guidance</th><th>Full Year FY23 Guidance</th></thead><thead><th>Revenue\")</th><th>$7.69 - $7.70 Billion</th><th>$31.7 - $31.8 Billion</th></thead><tr><td>Y/Y Growth</td><td>~21%</td><td>~20%</td></tr><tr><td>FX Impact?)</td><td>~($200M) y/y FX</td><td>~($600M) y/y FX®)</td></tr><tr><td>GAAP operating margin</td><td></td><td>~3.8%</td></tr><tr><td>Non-GAAP operating margin”)</td><td></td><td>~20.4%</td></tr><tr><td>GAAP earnings (loss) per share</td><td>($0.03) - ($0.02)</td><td>$0.38 - $0.40</td></tr><tr><td>Non-GAAP earnings per share</td><td>$1.01 - $1.02</td><td>$4.74 - $4.76</td></tr><tr><td>Operating Cash Flow Growth (Y/Y)</td><td></td><td>~21% - 22%</td></tr></table>'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements[-6].metadata.text_as_html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c269535-4fe9-44f2-8e0d-8578d706db7c",
   "metadata": {},
   "source": [
    "That table may look messy, but because it's in HTML format, the LLM is able to parse it much more easily than if it was tab or comma separated. You can copy and paste that html into a [html viewer](https://codebeautify.org/htmlviewer) online to see it reconstructed.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static/SalesforceFinancialTable.png\" alt=\"image\" style=\"max-width: 800px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1a5fe-245a-4d3c-86aa-03fca86ba5cf",
   "metadata": {},
   "source": [
    "Awesome, Unstructured was able to pull out the tables for us. It's not perfect, but the team is upgrading their toolset all the time.\n",
    "\n",
    "**Important Point:** Later on when we are doing semantic search over our chunks, trying to match on embeddings from the table directly will be difficult. A common practice that developers do is to *summarize* the table after you've extracted it. Then get an embedding of that summary. If the summary embedding matches what you're looking for, then pass the raw table to your LLM.\n",
    "\n",
    "### Multi-Modal (text + images)\n",
    "\n",
    "Next we'll dive into the world of multi-modal text splitting. This is a very active field and best practices are evolving. I'll show you a method that was made popular by [Lance Martin](https://twitter.com/RLanceMartin/status/1713638963255366091) of LangChain. You can check out his source code [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb). If you find a method that works better, share it out with the community!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dee2ed53-96c8-4cf3-89d9-681ff5d4552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install \"unstructured[all-docs]\"\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f20e1-e0fb-4d17-8ec7-faa4276fba23",
   "metadata": {},
   "source": [
    "First, let's go get a PDF to work with. This will be from a visual instruction tuning [paper](https://llava-vl.github.io/).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static/VisualInstructionSnapshot.png\" alt=\"image\" style=\"max-width: 800px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3f2d1771-7170-48c2-a614-480d7d2167df",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/VisualInstruction.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d9240942-2702-45ae-8333-8ac9c3e10343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=filepath,\n",
    "    \n",
    "    # Using pdf format to find embedded image blocks\n",
    "    extract_images_in_pdf=True,\n",
    "    \n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    \n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    # Hard max on chunks\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=\"static/pdfImages/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45aab5-5f51-4d67-9ea2-17c5489a5c2a",
   "metadata": {},
   "source": [
    "If you head over to `static/pdfImages/` and check out the images that were parsed.\n",
    "\n",
    "But the images don't do anything sitting in a folder, we need to do something with them! Though a bit outside the scope of chunking, let's talk about how to work with these.\n",
    "\n",
    "The common tactics will either use a multi-modal model to generate summaries of the images or use the image itself for your task. Others get embeddings of images (like [CLIP](https://openai.com/research/clip)).\n",
    "\n",
    "Let's generate summaries so you'll be inspired to take this to the next step. We'll use GPT-4V. Check out other models [here](https://platform.openai.com/docs/model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fc40dacd-09a0-4ce0-ae8c-87a3910a1408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7b48f-7da2-430f-a0dd-c8e1766854a3",
   "metadata": {},
   "source": [
    "We'll be using gpt-4-vision today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "59b1b6d2-4d84-41dc-8698-8be52e6f5bd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4-vision-preview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.env/lib/python3.10/site-packages/langchain_core/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.env/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-vision-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea28491-e225-4667-8666-3b0541dbf2b7",
   "metadata": {},
   "source": [
    "I'm creating quick helper function to convert the image from file to base64 so we can pass it to GPT-4V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f096a-abd1-4380-af24-6c65074d2420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert image to base64\n",
    "def image_to_base64(image_path):\n",
    "    with Image.open(image_path) as image:\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=image.format)\n",
    "        img_str = base64.b64encode(buffered.getvalue())\n",
    "        return img_str.decode('utf-8')\n",
    "\n",
    "image_str = image_to_base64(\"static/pdfImages/figure-15-6.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80becf02-83bd-4560-af3a-dece72259296",
   "metadata": {},
   "source": [
    "Then we can go ahead and pass our image to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d7f95-d181-4b4a-aeb0-b40367f0f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-4-vision-preview\",\n",
    "                  max_tokens=1024)\n",
    "\n",
    "msg = chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\" : \"Please give a summary of the image provided. Be descriptive\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_str}\"\n",
    "                    },\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67130242-5d72-4304-a705-c9177558a7d4",
   "metadata": {},
   "source": [
    "Then the summary returned is what we will put into our vectordata base. Then when it comes time to do our retrieval process, we'll use these embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c2833-ee05-43b2-9cd2-6710d6e73ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129deb0-7b11-48f6-bc3e-210658e9f8e4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static/pdfImages/figure-15-6.jpg\" alt=\"image\" style=\"max-width: 800px;\"><br>\n",
    "    <span><i>static/pdfImages/figure-15-6.jpg</i></span>\n",
    "</div>\n",
    "\n",
    "Hm, that seems about right!\n",
    "\n",
    "There are a ton of ways to go about this (check out the bonus section for more) so don't take my word for it - try 'em."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d18623d-6fc1-4b90-a016-ee79ac28b9ad",
   "metadata": {},
   "source": [
    "Then I want to split the entire essay into 1-sentence chunks. I'm going to split on \".\" \"?\" and \"!\". There are better ways to do this but this is quick and easy for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340df5c-bae9-4f04-a69a-4263c7e912de",
   "metadata": {},
   "source": [
    "But a list of sentences can be tough to add more data too. I'm going to turn this into a list of dictionaries (`List[dict]`), of which, the sentences will be a key-value. Then we can start to add more data to each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ff201-d657-49e0-8a03-8e6460113393",
   "metadata": {},
   "source": [
    "## 4: Agentic Chunking <a id=\"AgenticChunking\"></a>\n",
    "Taking level 4 even further - can we instruct an LLM to do this task like a human would?\n",
    "\n",
    "How does a human even go about chunking in the first place?\n",
    "\n",
    "Well...let me think, how would I go about chunking a document into its discrete parts such that the results were semantically similar?\n",
    "\n",
    "1. I would get myself a scratch piece of paper or notepad\n",
    "2. I'd start at the top of the essay and assume the first part will be a chunk (since we don't have any yet)\n",
    "3. Then I would keep going down the essay and evaluate if a new sentence or piece of the essay should be a part of the first chunk, if not, then create a new one\n",
    "4. Then keep doing that all the way down the essay until we got to the end.\n",
    "\n",
    "Woah! Wait a minute - this is pseudo code for something we can try out. See me tease this [here](https://twitter.com/GregKamradt/status/1738276097471754735).\n",
    "\n",
    "I debated whether or not to hold myself to the strict standard of using the *raw text* from a document, or use a derived form. The former felt like I was being too harsh, so I decided to explore using [propositions](https://twitter.com/LangChainAI/status/1735708004618764470). This is a cool concept ([research paper](https://arxiv.org/pdf/2312.06648.pdf)) that extracts stand alone statements from a raw piece of text.\n",
    "\n",
    "Example: `Greg went to the park. He likes walking` > `['Greg went to the park.', 'Greg likes walking']`\n",
    "\n",
    "Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f79c0-f6bd-4561-bf0a-434955d875e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.chains import create_extraction_chain\n",
    "from typing import Optional, List\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba10ec8-8227-4b95-a2cc-4149941dfe98",
   "metadata": {},
   "source": [
    "Pulling out propositions is done via a well-crafted prompt. I'm going to pull it from [LangHub](https://smith.langchain.com/hub?organizationId=50995362-9ea0-4378-ad97-b4edae2f9f22), LangChain's home for prompts.\n",
    "\n",
    "You can view the proposition prompt [here](https://smith.langchain.com/hub/wfh/proposal-indexing?organizationId=50995362-9ea0-4378-ad97-b4edae2f9f22).\n",
    "\n",
    "I'll use gpt-4 as the LLM because we aren't messing around. I care more about performance than I do speed or cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de38943-7697-413e-a2e8-8a62cda914b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = hub.pull(\"wfh/proposal-indexing\")\n",
    "llm = ChatOpenAI(model='gpt-4-1106-preview', openai_api_key = os.getenv(\"OPENAI_API_KEY\", 'YouKey'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a838bc4-e732-492d-80a5-8c829ca637ef",
   "metadata": {},
   "source": [
    "Then I'll make a runnable w/ langchain, this'll be a short way to combine the prompt and llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97236eb3-361e-4ebd-aa13-979ac8718f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use it in a runnable\n",
    "runnable = obj | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea6b054-bac6-4ce6-be8f-724a6ea1c5ef",
   "metadata": {},
   "source": [
    "The output from a runnable is a json-esque structure in a string. We need to pull the sentences out. I found that LangChain's example extraction was giving me a hard time so I'm doing it manually with a pydantic data class. There is definitely room to improve this.\n",
    "\n",
    "Create your class then put it in an extraction chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10116bfe-12a0-4f8c-95bf-67a0f8c1ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic data class\n",
    "class Sentences(BaseModel):\n",
    "    sentences: List[str]\n",
    "    \n",
    "# Extraction\n",
    "extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462b5c8-5cb2-4dcb-bedd-6ee1ce2cfd08",
   "metadata": {},
   "source": [
    "Then wrap it together in a function that'll return a list of propositions to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec04652-19e3-4cab-a882-7c31cd87fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propositions(text):\n",
    "    runnable_output = runnable.invoke({\n",
    "    \t\"input\": text\n",
    "    }).content\n",
    "    \n",
    "    propositions = extraction_chain.run(runnable_output)[0].sentences\n",
    "    return propositions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0bf288-24ba-4a50-a225-400c6fff5c61",
   "metadata": {},
   "source": [
    "Go get your text of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be92b95-76d6-4a2c-ba2f-67379998956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/PGEssays/superlinear.txt') as file:\n",
    "    essay = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4ee69-a87e-481e-b01d-403021a2610b",
   "metadata": {},
   "source": [
    "Then you need to decide what you send to your proposal maker. The prompt has an example that is about 1K characters long. So I would experiment with what works for you. This isn't another chunking decision, just pick something reasonable and try it out.\n",
    "\n",
    "I'm using paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121baaf-c65a-4dce-a942-83cfbf6891cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = essay.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ae4f7-e58b-4941-b243-ccc72c3bf37c",
   "metadata": {},
   "source": [
    "Let's see how many we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c36b2-bce3-461f-9f52-af1c10b1c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cb285-abf3-4334-aff7-5f84d741c7f4",
   "metadata": {},
   "source": [
    "That's too many for a demo, I'll do just the first couple to show it off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fddfc4-7cd6-4fe4-add7-7a12c593ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_propositions = []\n",
    "\n",
    "for i, para in enumerate(paragraphs[:5]):\n",
    "    propositions = get_propositions(para)\n",
    "    \n",
    "    essay_propositions.extend(propositions)\n",
    "    print (f\"Done with {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01906ad-b5a2-4b73-a079-4564b66b147e",
   "metadata": {},
   "source": [
    "Let's take a look at what the propositions look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867944e-13f0-4207-b787-692eeebadea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"You have {len(essay_propositions)} propositions\")\n",
    "essay_propositions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ebfc6-da6a-4fb8-833d-ed2e9810f25e",
   "metadata": {},
   "source": [
    "So you'll see that they look like regular sentences, but they are actually statements that are able to stand on their own. For example, one of the sentences in the raw text is \"They meant well, but this is rarely true.\" if you were to chunk that on it's own, the LLM would have no idea who you're talking about. Who meant well? What is rarely true? But those have been covered by the propositions.\n",
    "\n",
    "Now onto the cool part, we need a system that can reason about each proposition and determine whether or not it should be a part of an existing chunk or if a new chunk should be made.\n",
    "\n",
    "The pseudo code for how this works is above - I also review this code in the video so make sure to go watch that if you want to see me chat about it live.\n",
    "\n",
    "The script is also in this repo if you've cloned it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193f8e1-28aa-4c8c-8929-bfbcf7edc618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini script I made\n",
    "from agentic_chunker import AgenticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550431a-9484-4095-a888-31ec3cbe1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = AgenticChunker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833086aa-d1b7-4ac8-9d85-3a71e6e634ab",
   "metadata": {},
   "source": [
    "Then let's pass in our propositions to it. There are a lot in the full list so I'm only going to do a subset.\n",
    "\n",
    "This method is slow and expensive, but let's see how the results are.\n",
    "\n",
    "You can turn off the print statements via setting `ac = AgenticChunker(print_logging=False)` when you create your chunker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e9892a-9a1c-4269-a262-7cc4ab0eed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.add_propositions(essay_propositions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1481fe66-f73c-4fd2-a51c-3a1131a8e57c",
   "metadata": {},
   "source": [
    "Cool, looks like a few chunks were made. Let's check them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c79f2a-119e-45f2-8883-0f623a7c211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.pretty_print_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aebf75-53b9-4695-bea9-effa0adf967e",
   "metadata": {},
   "source": [
    "Awesome, then if we wanted to get the chunks properly, then we get extract a list of strings with them. The chunks propositions will be joined in the same string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e5eb4-676f-4310-8bbf-be87f94d07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = ac.get_chunks(get_type='list_of_strings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08eb17e-10ab-426e-8998-bed29ab76dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920b345-0294-4b5d-995c-da36bdfaeace",
   "metadata": {},
   "source": [
    "Great, now we can go use that in our evaluations for your retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8462b-8c10-4550-b42c-453ecdec3647",
   "metadata": {},
   "source": [
    "## 5: Alternative Representation <a id=\"BonusLevel\"></a>\n",
    "So far I've shown how to chunk up your raw text (okay, I was a bit liberal with level 5).\n",
    "\n",
    "But what if your raw text isn't the best way to represent your data for your task?\n",
    "\n",
    "For example, if you're doing semantic search on chat messages, raw chat messages may lack the context to make a successful embedding. Maybe actually trying to semantic search of a summary of a conversation would do better. Or maybe hypothetical questions that the chat would answer?\n",
    "\n",
    "This is where the world of chunking/splitting starts to dive into the world of [indexing](https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing.html#what-is-an-index). When you index, you're making a choice about how you want to represent your data in your data base or knowledge base.\n",
    "\n",
    "This is more of a retrieval topic, but it's worth talking about with chunking.\n",
    "\n",
    "Let's quickly go through a few popular alternative ways developers like to represent their data. There are unlimited methods to try. We'll review 4 of them\n",
    "\n",
    "* **[Multi-Vector Indexing](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector)** - This is when you do semantic search for a vector that is derived from something other than your raw text\n",
    "    * **Summaries** - A summary of your chunk\n",
    "    * **Hypothetical questions** - Good for chat messages used as knowledge base\n",
    "    * **Child Documents** - Parent Document Retriever\n",
    "* **Graph Based Chunking** - Transposing your raw text into a graph structure\n",
    "\n",
    "### Summaries\n",
    "Instead of embedding your raw text, embed a summary of your raw text which will have more dense information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9678d-187e-4293-b1aa-eda9045035ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445bc40-531b-4a65-b4ee-852377862ce9",
   "metadata": {},
   "source": [
    "Let's use our Super Linear essay again. I'll split it into large chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c1a25-aaff-4d74-a5fd-f681f7cf478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/PGEssays/superlinear.txt') as file:\n",
    "    essay = file.read()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=0)\n",
    "\n",
    "docs = splitter.create_documents([essay])\n",
    "\n",
    "print (f\"You have {len(docs)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b418bdb-c03c-4885-8a74-983c5732f6dc",
   "metadata": {},
   "source": [
    "Spin up a chain that will quickly summarize for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460da5b4-cd95-4ea5-ae21-57c2cf99b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | ChatOpenAI(max_retries=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67afe1-694f-47a5-acea-7a788ea5ba07",
   "metadata": {},
   "source": [
    "Then let's get the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6473c02-5728-418d-a201-5216d83599dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9282853-9a62-4ffd-a1bf-b0e8853ed864",
   "metadata": {},
   "source": [
    "Let's look at a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecc0c8f-3eb5-4e42-bd6e-2fdec56cc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa4786-1d43-463b-a63d-55e1a9f35af3",
   "metadata": {},
   "source": [
    "Then we are going to create a vectorstore (holds vectors + summaries) and a docstore (holds raw docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76c297-804f-4e5a-9fed-f28b88f57059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f44d2b-6a25-431c-be0b-72fff86e019c",
   "metadata": {},
   "source": [
    "Then you want to create documents out of your summary list, add the doc_id to it's metadata. This will tie it back to the original document so you know which summary goes with which original doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d5cb8-cfe5-4a89-bb9a-fb2872e41be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02878000-5341-42fc-a1b0-a7e9da3367d7",
   "metadata": {},
   "source": [
    "Then add them both to your vectorestore and docstore. When you add the docs to the vectorstore it will get the embeddings for them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53351f4d-f5db-4aeb-9b79-f4a0a3ba30f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the summaries\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "\n",
    "# Adds the raw documents\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761a8ae-d31c-483b-bdf4-4a625e3e9662",
   "metadata": {},
   "source": [
    "Then if you want you can add the original docs to the vectorstore as well. Just make sure to add the doc id to it as well so you can tie it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3b781-3166-4d91-ad03-48bc8b246d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, doc in enumerate(docs):\n",
    "#     doc.metadata[id_key] = doc_ids[i]\n",
    "# retriever.vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3397531-c8d3-4037-8f64-c51eeb6dc49b",
   "metadata": {},
   "source": [
    "Great, now that we've done all that work, let's try a search. If you run the code below, you'll search on the summary embeddings, but you'll get the raw documents returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9848e-136b-496f-b3ba-f57720a11993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.get_relevant_documents(query=\"the concept of superlinear returns, which refers to the idea that the returns for performance are not linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d6ab29-a89a-4259-847d-492ccbaad02f",
   "metadata": {},
   "source": [
    "### Hypothetical Questions\n",
    "You can generate hypothetical questions about your raw documents. Check out [LangChain's](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#hypothetical-queries) implementation of it for more information.\n",
    "\n",
    "This is espeically helpful when you have sparse unstructured data, like chat messages.\n",
    "\n",
    "Say you were to build a bot that uses slack conversations as a knowledge base. Trying to do semantic search on raw chat messages might not have the greatest results. However, if you were to generate hypothetical questions that the slack messages would answer, then when you get a new question in, you'll likely have a better chance matching.\n",
    "\n",
    "The code for this will be the same as the summary code, but instead of asking the LLM to make a summary, you'll ask it for hypothetical questions.\n",
    "\n",
    "### Parent Document Retriever (PDR)\n",
    "Much like the previous two, Parent Document Retriever builds on the concept of doing semantic search on a varied representation of your data.\n",
    "\n",
    "The hypothesis with the PDR is that smaller chunks have a higher likely hood of being matched semantically with a potential query. However, those smaller chunks may not have all the context they need, so instead of passing the smaller chunks to your LLM, get the parent chunk of the smaller chunk. This means you get a larger chunk which the smaller chunk is placed in.\n",
    "\n",
    "Check out LangChain's implementation's implementation of it [here](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever).\n",
    "\n",
    "\n",
    "I want to quickly go over a similar method in [Llama Index](https://www.llamaindex.ai/) with their `HierarchicalNodeParser` which will split a document at various chunk sizes (there will be a bunch of overlaps but that is the purpose). When combined with their `AutoMergingRetriever` you can do complicated retrieval easily. Their walkthrough [here](https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078afc3-ecea-4cf1-b430-e27837508e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import HierarchicalNodeParser\n",
    "\n",
    "node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128],\n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c19a08-6a94-45a4-8d31-1ad91e014030",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"../../data/PGEssays/mit.txt\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a9ecb0-c264-40cc-938a-3610c1a31835",
   "metadata": {},
   "source": [
    "Then let's do our splitting. There will be a bunch of chunks since we included `128` as a chunk size above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806e1a0-79df-418d-962d-3757081fd273",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad26bbc-1b67-45dc-b44e-d62e36fd3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"You have {len(nodes)} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d703f821-f2e2-4975-a904-893007bb256c",
   "metadata": {},
   "source": [
    "Then let's look at the relationships that are available to one of the small nodes at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c037ce3e-d480-43e6-be47-0e7415153480",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[-2].relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb0ed8-4427-4e8d-a8e9-9d11f3dcef9a",
   "metadata": {},
   "source": [
    "You can see there are source, previous, next, and parent. For [more information on these](https://docs.llamaindex.ai/en/stable/api/llama_index.schema.NodeRelationship.html).\n",
    "\n",
    "### Graph Structure\n",
    "If your data is rich with entities, relationships, and connections, then a graph structure may be best for you.\n",
    "\n",
    "Few options:\n",
    "* [Diffbot](https://www.diffbot.com/)\n",
    "* [InstaGraph](https://github.com/yoheinakajima/instagraph) - By [Yohei](https://twitter.com/yoheinakajima)\n",
    "\n",
    "I'll run through the LangChain supported version of Diffbot due to brevity. You'll need an API key from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ee2d2-f852-4f01-bc1b-ec5355f36e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install langchain langchain-experimental openai neo4j wikipedia\n",
    "\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "\n",
    "diffbot_nlp = DiffbotGraphTransformer(diffbot_api_key=os.getenv(\"DIFFBOT_API_KEY\", 'YourKey'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a7ffb-acdc-4fd3-b4ba-9ce4eb09bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Greg is friends with Bobby. San Francisco is a great city, but New York is amazing.\n",
    "Greg lives in New York. \n",
    "\"\"\"\n",
    "docs = [Document(page_content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41436339-39c6-4cf7-aba6-0a2dcafec00b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_documents = diffbot_nlp.convert_to_graph_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250215f0-2ba9-49e4-8aef-1751ccc1b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19567c26",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
